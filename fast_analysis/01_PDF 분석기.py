import os; import warnings; warnings.filterwarnings("ignore")           # ê²½ê³  ë©”ì‹œì§€ ë¬´ì‹œ
from dotenv import load_dotenv; load_dotenv()
import os; import pandas as pd; import numpy as np; import streamlit as st    
import matplotlib as plt; import seaborn as sns; import plotly   
import streamlit as st
from langchain_core.messages.chat import ChatMessage
from langchain_core.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.prompts import MessagesPlaceholder
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser
from langchain_core.runnables import RunnablePassthrough
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.runnables.history import RunnableWithMessageHistory

from langchain_teddynote import logging           
logging.langsmith("[Project] PDF ë©€í‹°í„´ RAG ì±—ë´‡")

# ë©€í‹°í„´
from operator import itemgetter
from retriever import create_retriever
from langchain_core.runnables import RunnableLambda

from langchain_community.vectorstores import FAISS
from langchain_openai import OpenAIEmbeddings

# ìºì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
if not os.path.exists(".cache"):
    os.mkdir(".cache")

if not os.path.exists(".cache/embeddings"):
    os.mkdir(".cache/embeddings")

if not os.path.exists(".cache/files"):
    os.mkdir(".cache/files")

########################################
# PDF RAG + ë©€í‹°í„´ ì±—ë´‡
########################################

st.title("PDF ê¸°ë°˜ ë©€í‹°í„´ ì±—ë´‡ğŸ“‘")

# ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ ì´ˆê¸°í™”
if "messages_pdf" not in st.session_state:
    st.session_state["messages_pdf"] = []

if "store_pdf" not in st.session_state:
    st.session_state["store_pdf"] = {}

if "chain_pdf" not in st.session_state:
    st.session_state["chain_pdf"] = None

################## ì‚¬ì´ë“œë°” ##################
with st.sidebar:
    # ì´ˆê¸°í™” ë²„íŠ¼
    clr_btn = st.button("ëŒ€í™” ì´ˆê¸°í™”")

    # íŒŒì¼ ì—…ë¡œë“œ
    uploaded_file = st.file_uploader("PDF íŒŒì¼ ì—…ë¡œë“œ", type=['pdf'])

    # ëª¨ë¸ ì„ íƒ
    selected_model = st.selectbox(
        "ëª¨ë¸ ì„ íƒ", ["gpt-4o", "gpt-4-turbo", "gpt-4o-mini"], index=0
    )

    # ì„¸ì…˜ ID ì…ë ¥
    session_id = st.text_input("ì„¸ì…˜ IDë¥¼ ì…ë ¥í•˜ì„¸ìš”", "abc123")

    # ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸ ì…ë ¥ë°›ê¸°
    system_prompt = st.text_area("ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸", 
    """ë‹¹ì‹ ì€ ì§ˆë¬¸ ë‹µë³€ ì‘ì—…ì˜ ë³´ì¡°ìì…ë‹ˆë‹¤.\
ê²€ìƒ‰ëœ ì»¨í…ìŠ¤íŠ¸ì™€ ì‚¬ìš©ì ì…ë ¥ì˜ ë‹¤ìŒ ë¶€ë¶„ì„ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì‹­ì‹œì˜¤.\
ë‹µì„ ëª¨ë¥¸ë‹¤ë©´ ê·¸ëƒ¥ ëª¨ë¥¸ë‹¤ê³  ë§í•˜ì‹­ì‹œì˜¤. í•œêµ­ì–´ë¡œ ë‹µë³€í•˜ì„¸ìš”.""",
height=200,) 
    
    # ë°ì´í„° ë¶„ì„ì„ ì‹œì‘í•˜ëŠ” ë²„íŠ¼
    apply_btn = st.button("ë°ì´í„° ë¶„ì„ ì‹œì‘")  
    
################## /ì‚¬ì´ë“œë°” ##################

################################ ê¸°ëŠ¥í•¨ìˆ˜ ì •ì˜ ################################

# ì´ì „ ëŒ€í™” ì¶œë ¥ í•¨ìˆ˜
def print_messages():
    for chat_message in st.session_state["messages_pdf"]:
        st.chat_message(chat_message.role).write(chat_message.content)

# ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ë©”ì‹œì§€ ì¶”ê°€
def add_message(role, message):
    st.session_state["messages_pdf"].append(ChatMessage(role=role, content=message))

# ì„¸ì…˜ IDë³„ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
def get_session_history(session_ids):
    if session_ids not in st.session_state["store_pdf"]:
        st.session_state["store_pdf"][session_ids] = ChatMessageHistory()
    return st.session_state["store_pdf"][session_ids]

# íŒŒì¼ ì—…ë¡œë“œ ì‹œ retriever ìƒì„±
@st.cache_resource(show_spinner="ì—…ë¡œë“œí•œ íŒŒì¼ì„ ì²˜ë¦¬ ì¤‘ì…ë‹ˆë‹¤...")
def embed_file(file):
    file_content = file.read()
    file_path = f"./.cache/files/{file.name}"
    with open(file_path, "wb") as f:
        f.write(file_content)
        
    return create_retriever(file_path)

# ë¬¸ì„œë¥¼ í¬ë§·íŒ…í•˜ëŠ” í•¨ìˆ˜
def format_docs(docs):
    return "\n".join([doc.page_content for doc in docs])

# ë©€í‹°í„´ RAG ê¸°ë°˜ ì²´ì¸ ìƒì„± í•¨ìˆ˜
def create_chain(retriever, model_name='gpt-4o'):
    
    prompt = ChatPromptTemplate.from_messages(
        [
            (
                "system",
                ""
            ),
            # ëŒ€í™”ê¸°ë¡ìš© key ì¸ chat_history ëŠ” ê°€ê¸‰ì  ë³€ê²½ ì—†ì´ ì‚¬ìš©í•˜ì„¸ìš”!
            MessagesPlaceholder(variable_name="chat_history"),
            ("human", "# Context:\n {context} \n\n # Question:\n{question} # Answer:"),  # ì‚¬ìš©ì ì…ë ¥ì„ ë³€ìˆ˜ë¡œ ì‚¬ìš©
        ]
    )

    llm = ChatOpenAI(model_name='gpt-4o', temperature=0)

    chain = (
    {
        "context": itemgetter("question") | retriever,
        "question": itemgetter("question"),
        "chat_history": itemgetter("chat_history"),
    }
    | prompt 
    | llm 
    | StrOutputParser()
    )
    
    # RunnableWithMessageHistoryë¥¼ ì‚¬ìš©í•˜ì—¬ ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬
    chain_with_history = RunnableWithMessageHistory(
        chain,
        get_session_history,  # ì„¸ì…˜ë³„ ëŒ€í™” ê¸°ë¡ ê´€ë¦¬ í•¨ìˆ˜
        input_messages_key="question",
        history_messages_key="chat_history",
    )
    return chain_with_history

################################ /ê¸°ëŠ¥í•¨ìˆ˜ ì •ì˜ ################################

# ì´ˆê¸°í™” ë²„íŠ¼ ë™ì‘
if clr_btn:
    st.session_state["messages_pdf"] = []

# íŒŒì¼ ì—…ë¡œë“œ í›„ ëŒ€í™” ì‹œì‘ ë²„íŠ¼ ëˆ„ë¥´ê¸°
if apply_btn and uploaded_file:
    st.success("ì„¤ì •ì´ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤. ëŒ€í™”ë¥¼ ì‹œì‘í•´ ì£¼ì„¸ìš”!")
elif apply_btn:
    st.warning("ë¨¼ì € íŒŒì¼ì„ ì—…ë¡œë“œ í•´ì£¼ì„¸ìš”.")

# ì´ì „ ëŒ€í™” ê¸°ë¡ ì¶œë ¥
print_messages()

# ê²½ê³  ë©”ì‹œì§€ë¥¼ ë„ìš°ê¸° ìœ„í•œ ë¹ˆ ì˜ì—­
warning_msg = st.empty()

# íŒŒì¼ ì—…ë¡œë“œê°€ ë˜ë©´ retriever ìƒì„±, chain ìƒì„±, ì„¸ì…˜ ìŠ¤í…Œì´íŠ¸ì— ì €ì¥
if uploaded_file:
    retriever = embed_file(uploaded_file)
    chain = create_chain(retriever)
    st.session_state["chain_pdf"] = chain


# ì‚¬ìš©ì ì…ë ¥
user_input = st.chat_input("ë¬´ì—‡ì´ ê¶ê¸ˆí•˜ì‹ ê°€ìš”?")

# ì‚¬ìš©ì ì…ë ¥ì´ ë“¤ì–´ì˜¤ë©´
if user_input:
    chain = st.session_state["chain_pdf"]

    # ì²´ì¸ì´ ìƒì„±ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸
    if chain is not None:
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶œë ¥
        st.chat_message("user").write(user_input)
        
        # ë‹µë³€ ìƒì„± (ì„¸ì…˜ IDë¥¼ í†µí•œ ëŒ€í™” ê´€ë¦¬)
        response = chain.stream(    # stream ì•„ì›ƒí’‹ì€ ["ì•ˆ", "ë…•"] ë¦¬ìŠ¤íŠ¸ì²˜ëŸ¼ ê°œë³„ ë‹¨ìœ„ë¡œ ì œê³µë©ë‹ˆë‹¤.
            {"question": user_input},
            config={"configurable": {"session_id": session_id}},
        )

        with st.chat_message("assistant"):
            # ë¹ˆ ê³µê°„(ì»¨í…Œì´ë„ˆ)ì„ ë§Œë“¤ì–´ì„œ, ì—¬ê¸°ì— í† í°ì„ ìŠ¤íŠ¸ë¦¬ë° ì¶œë ¥í•œë‹¤.
            container = st.empty()

            ai_answer = ""
            for token in response:
                # print(token)
                ai_answer += token
                # print(ai_answer)
                container.markdown(ai_answer)   
            print(ai_answer)

            # ëŒ€í™”ê¸°ë¡ì„ ì €ì¥í•œë‹¤.
            add_message("user", user_input)
            add_message("assistant", ai_answer)
